# -*- coding:utf-8 -*-

import json
import os
import shutil
import jieba
import sys
import re
from importlib import reload
import os

dirname = os.path.dirname(__file__)
data_set = os.path.join(dirname, '../dataset/raw.txt')
target = os.path.join(dirname, '../dataset/target.txt')
stopwords_dict = os.path.join(dirname, '../dataset/stop_words_ch.txt')

with_evidence = False

emoji_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"  # emoticons
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                u"\u0000-\u002f"
                u"\u003a-\u0040"
                u"\u005b-\u0060"
                u"\u007b-\u4dff"
                u"\u9FA6-\uffff"
                u"\u0669"
                u"\u06f6"
                u"\u0e51"
                u"\u0f40"
                u"\u0f0b"
                u"\u141b"
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u2200"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\u30fb"
                u"\ufe0f"
                u"\uff65"
                u"\uff89"
                u"\uff9f"
                u"\xa0"
                u"\xb4"
                u"\xbf"
    "]+", flags=re.UNICODE)


def rm_stopwords(file_path, word_dict):
    """
        rm stop word for {file_path}, stop words save in {word_dict} file.
        file_path: file path of file generated by function splitwords.
                    each lines of file is format as <file_unique_id> <file_words>.
        word_dict: file containing stop words, and every stop words in one line.
        output: file_path which have been removed stop words and overwrite original file.
    """

    # read stop word dict and save in stop_dict
    stop_dict = {}
    with open(word_dict, "r", encoding='utf-8') as d:
        for word in d:
            stop_dict[word.strip("\n")] = 1
    # remove tmp file if exists
    if os.path.exists(file_path + ".tmp"):
        os.remove(file_path + ".tmp")

    print ("now remove stop words in %s." % file_path)
    # read source file and rm stop word for each line.
    with open(file_path, 'r', encoding='utf-8') as f1, open(file_path + ".tmp", "w", encoding='utf-8') as f2:
        for line in f1:
            tmp_list = []  # save words not in stop dict
            words = line.split()
            for word in words:
                if word not in stop_dict:
                    tmp_list.append(word)
            words_without_stop = " ".join(tmp_list)
            to_write = words_without_stop + "\n"
            f2.write(to_write)

    # overwrite origin file with file been removed stop words
    shutil.move(file_path + ".tmp", file_path)
    print ("stop words in %s has been removed." % file_path)


shouldSkipNext = False
removeWords = ["雀魂友人房", "傻逼", "死妈", "http"]

# 去复读机
all_data = set()

with open(data_set, "r", encoding='utf-8') as f, open(target, "w", encoding='utf-8') as f2:
    count = 0
    for line in f:
        print("now processing line %s" % count)
        line = line.replace('Phoenix', '')
        line = line.strip('\n').replace('[图片]', '')#去除换行符，并在字符间添加空格符，原因是用于区分 123 与1 2 3.
        if line.startswith('#ky'):
            continue

        line = emoji_pattern.sub(r'', line)

        if line.startswith('201') and ("925490088" in line or "10000" in line): # bot
            shouldSkipNext = True
            continue
        if line.startswith('201'):
            shouldSkipNext = False
            continue
        if shouldSkipNext == True:
            continue
        if any(wd in line for wd in removeWords):
            continue
        if len(line) > 4:
            if line not in all_data:
                words = jieba.cut(line, cut_all=False)
                f2.write(" ".join(words) + "\n")
                count += 1
                all_data.add(line)



    print ("all question num is %s" % count)
    print("set length is %s" % len(all_data))

rm_stopwords(target, stopwords_dict)
